<!DOCTYPE html>
<html lang="zh-CN">
	<head>
		
<title>中间件源码</title>
<meta charset="utf-8" />
<meta name="keywords" content="" />
<meta
    name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=5"
/>
<meta name="generator" content="Hexo 6.2.0">
<link rel="stylesheet" href="/css/style.css?v=1664202850875">
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Noto+Serif+SC&display=swap"
    media="all"
/>
<link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/solarized-light.css"
/>
<script src="/js/core.js?v=1664202850875"></script>






    <!-- baidu Analytics -->
<script>
    var _hmt = _hmt || [];
    (function() {var hm = document.createElement('script');
    hm.src = 'https://hm.baidu.com/hm.js?2ee30baeebf59e698360da94012edd15';
    var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

<script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.1/lazysizes.min.js" async></script>
<!--<script src="" async></script>--> 
	
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="中间件源码" type="application/atom+xml">
</head>

	<body>
        <div class="header  ">
    <div class="container">
        <a class="logo" href="/" title="中间件源码">中间件源码</a>
        <ul class="nav">
            
                <li class="nav-item"><a href="/index.html">首页</a></li>
            
                <li class="nav-item"><a href="/categories">分类</a></li>
            
        </ul>
    </div>
</div>
        <div class="content">
	<div class="banner">
		<div class="container">
			<h1></h1>
			<div class="info"><span class="date">2022年9月15日</span>•songxiaosheng 
			
				<!-- 
					<a href="https://github.com/nexmoe/books/tree/master/source/_posts/linux/net/2-网络排查.md" target="_blank" rel="external nofollow noreferrer noopener">编辑</a>
				 -->
			</div>
			
		</div>
	</div>
	<div class="container">
		<article class="post">
			<p>TCP 连接的建立是一个从 Client 侧调用 connect()， 到 Server 侧 accept() 成功返回的过程。你可以看到，在整个 TCP 建立连接的过程中，各 个行为都有配置选项来进行控制。</p>
<p><strong>重传配置</strong></p>
<p>Client 会给 Server 发送一个 SYN 包，但是该 SYN 包可能会在传输过程中丢失，或 者因为其他原因导致 Server 无法处理，此时 Client 这一侧就会触发超时重传机制。但是 也不能一直重传下去，重传的次数也是有限制的，这就是 tcp_syn_retries 这个配置项来决 定的。</p>
<p>对于 tcp_syn_retries 为 3 而言，总共会重传 3 次，也就是说从第一次发出 SYN 包后，会 一直等待(1 + 2 + 4 + 8)秒，如果还没有收到 Server 的响应，connect() 就会产生 ETIMEOUT 的错误。</p>
<p>tcp_syn_retries 的默认值是 6，也就是说如果 SYN 一直发送失败，会在(1 + 2 + 4 + 8 + 16+ 32 + 64)秒，即 127 秒后产生 ETIMEOUT 的错误。</p>
<p>通常情况下，我们都会将数据中心内部服务器的 tcp_syn_retries 给调小，这里推荐 设置为 2，来减少阻塞的时间。配置项为net.ipv4.tcp_syn_retries = 2</p>
<p><strong>半连接</strong></p>
<p>Server 没有响应 Client 的 SYN，除了我们刚才提到的 Server 已经不存在了这种情况 外，还有可能是因为 Server 太忙没有来得及响应，或者是 Server 已经积压了太多的半连 接(incomplete)而无法及时去处理。</p>
<p>半连接，即收到了 SYN 后还没有回复 SYNACK 的连接，Server 每收到一个新的 SYN 包，都会创建一个半连接，然后把该半连接加入到半连接队列(syn queue)中。syn queue 的长度就是 tcp_max_syn_backlog 这个配置项来决定的，当系统中积压的半连接 个数超过了该值后，新的 SYN 包就会被丢弃。对于服务器而言，可能瞬间会有非常多的新 建连接，所以我们可以适当地调大该值，以免 SYN 包被丢弃而导致 Client 收不到 SYNACK:</p>
<p>net.ipv4.tcp_max_syn_backlog = 16384</p>
<p>Server 中积压的半连接较多，也有可能是因为有些恶意的 Client 在进行 SYN Flood 攻 击。为了防止 SYN Flood 攻击，Linux 内核引入了 SYN Cookies 机制</p>
<p>推荐开启 SYN Cookies:   net.ipv4.tcp_syncookies = 1</p>
<p><strong>全连接</strong></p>
<p>就像半连接队列(syn queue)的长度有限制一样，全连接队列(accept queue) 的长度也有限制，目的就是为了防止 Server 不能及时调用 accept() 而浪费太多的系统资 源。</p>
<p>全连接队列(accept queue)的长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的，而该 backlog 的最大值则是 somaxconn。somaxconn 在 5.4 之前的内核中， 默认都是 128(5.4 开始调整为了默认 4096)，建议将该值适当调大一些: net.core.somaxconn = 16384</p>
<p><strong>tcp_abort_on_overflow</strong></p>
<p>当服务器中积压的全连接个数超过该值后，新的全连接就会被丢弃掉。Server 在将新连接 丢弃时，有的时候需要发送 reset 来通知 Client，这样 Client 就不会再次重试了。不过， 默认行为是直接丢弃不去通知 Client。至于是否需要给 Client 发送 reset，是由 tcp_abort_on_overflow 这个配置项来控制的，该值默认为 0，即不发送 reset 给 Client。推荐也是将该值配置为 0:net.ipv4.tcp_abort_on_overflow = 0</p>
<p>设置为0的原因：Server 如果来不及 accept() 而导致全连接队列满，这往往是由瞬间有大量新建 连接请求导致的，正常情况下 Server 很快就能恢复，然后 Client 再次重试后就可以建连 成功了。也就是说，将 tcp_abort_on_overflow 配置为 0，给了 Client 一个重试的机会。 当然，你可以根据你的实际情况来决定是否要使能该选项</p>
<p><strong>tcp_fin_timeout</strong></p>
<p>调用 close() 的一侧是 active close(主动关闭);而接收到对端的 FIN 包后再调用 close() 来关闭的一侧，称之为 passive close(被动关闭)</p>
<p>FIN_WAIT_2 状态，TCP 进入到这个状态后，如果本端迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源。Linux 为了防止这种资源 的开销，设置了这个状态的超时时间 tcp_fin_timeout，默认为 60s，超过这个时间后就会 自动销毁该连接。</p>
<p><strong>TIME_WAIT</strong></p>
<p>TIME_WAIT 状态，TIME_WAIT 状态存在的意义是:最后发送的这个 ACK 包 可能会被丢弃掉或者有延迟，这样对端就会再次发送 FIN 包。如果不维持 TIME_WAIT 这 个状态，那么再次收到对端的 FIN 包后，本端就会回一个 Reset 包，这可能会产生一些异 常。所以维持 TIME_WAIT 状态一段时间，可以保障 TCP 连接正常断开。TIME_WAIT 的默认 存活时间在 Linux 上是 60s(TCP_TIMEWAIT_LEN)，这个时间对于数据中心而言可能还 是有些长了，所以有的时候也会修改内核做些优化来减小该值，或者将该值设置为可通过 sysctl 来调节</p>
<p>TIME_WAIT 状态存在这么长时间，也是对系统资源的一个浪费，所以系统也有配置项来限 制该状态的最大个数，该配置选项就是 tcp_max_tw_buckets。对于数据中心而言，网络 是相对很稳定的，基本不会存在 FIN 包的异常，所以建议将该值调小一些:<br>net.ipv4.tcp_max_tw_buckets = 10000</p>
<p>Client 关闭跟 Server 的连接后，也有可能很快再次跟 Server 之间建立一个新的连接，而 由于 TCP 端口最多只有 65536 个，如果不去复用处于 TIME_WAIT 状态的连接，就可能 在快速重启应用程序时，出现端口被占用而无法创建新连接的情况。所以建议你打开复用 TIME_WAIT 的选项:<br>net.ipv4.tcp_tw_reuse = 1</p>
<p>还有另外一个选项 tcp_tw_recycle 来控制 TIME_WAIT 状态，但是该选项是很危险的，因 为它可能会引起意料不到的问题，比如可能会引起 NAT 环境下的丢包问题。所以建议将该 选项关闭:<br>net.ipv4.tcp_tw_recycle = 0</p>
<h2 id="发送问题"><a href="#发送问题" class="headerlink" title="发送问题"></a>发送问题</h2><h3 id="TCP发送缓冲区配置"><a href="#TCP发送缓冲区配置" class="headerlink" title="TCP发送缓冲区配置"></a>TCP发送缓冲区配置</h3><p>TCP 发送缓冲区的大小默认是受 net.ipv4.tcp_wmem 来控制:</p>
<p>tcp_wmem 中这三个数字的含义分别为 min、default、max。TCP 发送缓冲区的大小会 在 min 和 max 之间动态调整，初始的大小是 default，这个动态调整的过程是由内核自动 来做的</p>
<p>tcp_wmem 中的 max 不能超过 net.core.wmem_max 这个配置项的值，如果超过了， TCP 发送缓冲区最大就是 net.core.wmem_max</p>
<p> TCP 发送缓冲区太小，也会导致 业务延迟很大的问题</p>
<p>可以通过 setsockopt(2) 里的 SO_SNDBUF 来设置固定的缓冲区大小。一旦进行了这种设置后，tcp_wmem 就会失效，而且这个缓冲区大小设置的是固定值，内核也不会 对它进行动态调整。</p>
<p>SO_SNDBUF 设置的最大值不能超过 net.core.wmem_max，如果超过了该值，内 核会把它强制设置为 net.core.wmem_max</p>
<p>TCP 连接消耗的总内存也有限制:<br>net.ipv4.tcp_mem = 8388608 12582912 16777216</p>
<p>该选项中这些值的单位是 Page(页数)，也就是 4K。它也有 3 个值:min、pressure、max。当所有 TCP 连接消 耗的内存总和达到 max 后，也会因达到限制而无法再往外发包</p>
<p> tcp_mem 达到限制而无法发包或者产生抖动的问题，我们也是可以观测到的观察时你只需要打开 tracepiont(需要 4.16+ 的 内核版本):<br>$ echo 1 &gt; /sys/kernel/debug/tracing/events/sock/sock_exceed_buf_limit/enable<br>然后去看是否有该事件发生:<br>$ cat /sys/kernel/debug/tracing/trace_pipe<br>如果有日志输出(即发生了该事件)，就意味着你需要调大 tcp_mem 了，或者是需要断 开一些 TCP 连接了。</p>
<h3 id="IP-层"><a href="#IP-层" class="headerlink" title="IP 层"></a>IP 层</h3><p>net.ipv4.ip_local_port_range 这个配置选项，它是指和其他服务器建立 IP 连接时本地端 口(local port)的范围</p>
<p>net.ipv4.ip_local_port_range = 1024 65535</p>
<p>为了能够对 TCP/IP 数据流进行流控，Linux 内核在 IP 层实现了 qdisc(排队规则)</p>
<p>$ ip -s -s link ls dev eth0<br>…<br>TX: bytes packets errors dropped carrier collsns 3263284 25060 0 0 0 0<br>如果观察到 dropped 这一项不为 0，那就有可能是 txqueuelen 太小导致的。当遇到这种 情况时，你就需要增大该值了</p>
<p>txqueuelen 太小也会导致数据包被丢弃的 情况</p>
<h2 id="TCP-数据包的接收"><a href="#TCP-数据包的接收" class="headerlink" title="TCP 数据包的接收"></a>TCP 数据包的接收</h2><h3 id="TCP-硬中断poll数据大小"><a href="#TCP-硬中断poll数据大小" class="headerlink" title="TCP 硬中断poll数据大小"></a>TCP 硬中断poll数据大小</h3><p>TCP 数据包的接收流程在整体上与发送流程类似，只是方向是相反的。 数据包到达网卡后，就会触发中断(IRQ)来告诉 CPU 读取这个数据包。但是在高性能网 络场景下，数据包的数量会非常大，如果每来一个数据包都要产生一个中断，那 CPU 的处 理效率就会大打折扣，所以就产生了 NAPI(New API)这种机制让 CPU 一次性地去轮询 (poll)多个数据包，以批量处理的方式来提升效率，降低网卡中断带来的性能开销。<br>那在 poll 的过程中，一次可以 poll 多少个呢?这个 poll 的个数可以通过 sysctl 选项来控 制:<br>net.core.netdev_budget = 600<br>该控制选项的默认值是 300，在网络吞吐量较大的场景中，我们可以适当地增大该值，比 如增大到 600。增大该值可以一次性地处理更多的数据包。但是这种调整也是有缺陷的， 因为这会导致 CPU 在这里 poll 的时间增加，如果系统中运行的任务很多的话，其他任务 的调度延迟就会增加。</p>
<h3 id="TCP-接收缓冲区"><a href="#TCP-接收缓冲区" class="headerlink" title="TCP 接收缓冲区"></a>TCP 接收缓冲区</h3><p>默认都是使 用 tcp_rmem 来控制缓冲区的大小。同样地，我们也会适当地增大这几个值的默认值，来 获取更好的网络性能，调整为如下数值:<br>net.ipv4.tcp_rmem = 8192 87380 16777216</p>
<p>3 个字段:min、default、max。TCP 接收缓冲区大小也是在 min 和 max 之间动 态调整 ，不过跟发送缓冲区不同的是，这个动态调整是可以通过控制选项来关闭的，这个 选项是 tcp_moderate_rcvbuf 。通常我们都是打开它，这也是它的默认值:<br>net.ipv4.tcp_moderate_rcvbuf = 1</p>
<p>之所以接收缓冲区有选项可以控制自动调节，而发送缓冲区没有，那是因为 TCP 接收缓冲 区会直接影响 TCP 拥塞控制，进而影响到对端的发包，所以使用该控制选项可以更加灵活 地控制对端的发包行为</p>
<p>SO_RCVBUF 这个标记，那么 TCP 接收缓冲区的动态调整就是关闭，即使 tcp_moderate_rcvbuf 为 1，接收缓冲区的大小始终就为设置的 SO_RCVBUF 这个值</p>
<p>SO_RCVBUF 设置的值最大也不能超过 net.core.rmem_max。通常情况下，我们也需要设置 net.core.rmem_max 的值大于等于 net.ipv4.tcp_rmem 的 max:<br>net.core.rmem_max = 16777216</p>
<p>TCP 接收缓冲区的限制而引发的丢包问题。但是这 类问题不是那么好追踪的</p>
<p>  ![image-20220920080639876](/Users/mac/Library/Application Support/typora-user-images/image-20220920080639876.png)</p>
<p>![image-20220921074809241](/Users/mac/Library/Application Support/typora-user-images/image-20220921074809241.png)</p>
<p>引起 TCP 重传的情况在整体上可以分为如下两类</p>
<p>丢包<br>TCP 数据包在网络传输过程中可能会被丢弃;<strong>接收端</strong>也可能会把该数据包给丢弃;接收 端回的 ACK 也可能在<strong>网络传输过程中</strong>被丢弃;数据包在传输过程中<strong>发生错误而被接收端</strong> 给丢弃……这些情况都会导致发送端重传该 TCP 数据包。</p>
<p>拥塞<br>TCP 数据包在网络传输过程中可能会在某个交换机 / 路由器上排队，比如臭名昭著的 <strong>Bufferbloat(缓冲膨胀)</strong>;TCP 数据包在网络传输过程中因为<strong>路由变化而产生的乱序;</strong> 接收端回的 ACK 在某个交换机 / 路由器上排队……这些情况都会导致发送端再次重传该 TCP 数据包。</p>

			<br>
			
		</article>
	</div>
	<div class="other">
		<div class="container">
			<nav class="post-nav">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

</nav> 
		</div>
	</div>
	<div class="container comment">
		<div class="valine"></div>
<script src='https://cdn.jsdelivr.net/npm/valine'></script>
<script>
    // 使用方法 https://valine.js.org/quickstart.html
    new Valine({
        el: '.valine',
        appId: 'OrwF9gExPcH3pyeb35WIuKun-9Nh9j0Va',
        appKey: 'W098QAVm3hxbnBWRpho3a6HN'
    })
</script>
	</div>
</div>
		<div class="footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-left">中间件源码</div>
            <div class="footer-right"> 
                <div class="footer-links">
                    
                </div>
                <div calss="footer-copyright">&copy; 2022 中间件源码
                    Using <a rel="noreferrer" href="http://hexo.io/" target="_blank">Hexo</a> 
                    &amp; <a rel="noreferrer" href="https://github.com/Yet-The-Books/hexo-theme-yet-the-books" target="_blank">Yet The Books</a>
                </div>
            </div>  
        </div>
    </div>
</div>
	</body>
</html>
